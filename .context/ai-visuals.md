# AI-Powered Visual Generation

## Overview

AI visuals add a layer of imagery and artistic style that pure procedural generation cannot achieve. The system uses a tiered approach: Tier 1 (procedural, no AI), Tier 2 (AI keyframes with procedural interpolation), and Tier 3 (full AI video generation). This document details Tiers 2 and 3.

---

## Tier 2: AI Keyframes + Procedural Interpolation

### Concept

Generate a small number of AI images at structural keypoints in the song (section boundaries, emotional peaks). Use procedural effects (zoom, pan, blend, color shift) synchronized to beats to animate between keyframes. This delivers the AI aesthetic without per-frame generation cost.

### How Many Keyframes?

| Song Element | Keyframes Generated |
|-------------|-------------------|
| Each unique section (verse, chorus, bridge, intro, outro) | 1-2 images |
| Major emotional shifts within a section | 1 additional image |
| Climax / peak moment | 1 hero image |
| **Typical total for a 4-minute song** | **10-20 images** |

### Keyframe Generation Pipeline

```
Render Spec (per section)
     │
     ├── Section label: "chorus_1"
     ├── Visual theme: "cosmic energy, exploding stars"
     ├── Color palette: ["#FF6B35", "#F7C59F", "#004E89", "#1A1A2E"]
     ├── Mood: "euphoric, powerful"
     └── AI prompt: (generated by LLM)
           │
           ▼
     Prompt Engineering
           │
           ├── Base prompt from LLM suggestion
           ├── + Style modifiers (from template)
           ├── + Color enforcement
           ├── + Negative prompt (no text, no faces, no borders)
           ├── + Aspect ratio matching export preset
           │
           ▼
     Image Generation API
           │
           ├── DALL-E 3 (default for MVP)
           │     POST https://api.openai.com/v1/images/generations
           │     model: "dall-e-3"
           │     size: "1792x1024" (16:9) or "1024x1792" (9:16)
           │
           ├── Stable Diffusion (via ComfyUI API)
           │     Workflow: txt2img → upscale → color match
           │     Model: SDXL or Flux
           │     Steps: 30, CFG: 7.5
           │
           └── Stability AI API (alternative)
                 POST https://api.stability.ai/v2beta/stable-image/generate/ultra
           │
           ▼
     Generated Image
           │
           ├── Color grading (match to section palette)
           ├── Resolution scaling (match export resolution)
           └── Store in object storage
```

### Prompt Engineering

The LLM generates an initial prompt per section. Before sending to the image generator, the system applies these modifications:

```python
def build_image_prompt(section: SectionSpec, global_style: str) -> str:
    base = section.ai_prompt  # from LLM suggestion

    # Add style consistency
    style_suffix = f", {global_style} style, cohesive visual theme"

    # Add color guidance
    colors = ", ".join(section.color_palette)
    color_suffix = f", color palette: {colors}"

    # Add technical quality
    quality_suffix = ", high quality, detailed, 8k resolution, cinematic lighting"

    # Negative prompt (for SD-based generators)
    negative = "text, watermark, logo, signature, blurry, low quality, faces, people, borders, frame"

    return {
        "prompt": f"{base}{style_suffix}{color_suffix}{quality_suffix}",
        "negative_prompt": negative,
    }
```

### Procedural Interpolation Between Keyframes

Between AI keyframes, procedural effects create smooth motion:

| Effect | Parameters | Synced To |
|--------|-----------|-----------|
| **Ken Burns** (slow zoom + pan) | Zoom speed: 0.5-2%/sec, pan direction | Section energy level |
| **Cross-dissolve** | Duration: 1-4 beats | Section transitions |
| **Color pulse** | Intensity, color | Beat onsets |
| **Parallax layers** | Depth separation | Bass energy |
| **Film grain / noise** | Intensity | Overall energy |
| **Bloom / glow** | Radius, intensity | RMS loudness |
| **Chromatic aberration** | Offset amount | Spectral flux (on high-change moments) |
| **Morph blend** | Two keyframes blend via shader | Section transitions |

### Visual Consistency

To maintain visual coherence across keyframes:

1. **Consistent style prompt**: All keyframes share a global style modifier (e.g., "oil painting style", "cyberpunk neon aesthetic")
2. **Color palette enforcement**: Post-process generated images to match the section's palette using color grading
3. **Seed pinning** (SD only): Use related seeds for adjacent sections to maintain some visual continuity
4. **ControlNet** (SD only): Use depth maps or edge maps from the previous keyframe to guide the next

---

## Tier 3: Full AI Video Generation (Phase 2)

### Concept

Generate fully AI-animated video sequences using models like Deforum, AnimateDiff, or dedicated video generation models. Each section becomes a short AI-generated clip, stitched together with transitions.

### Deforum Integration

Deforum animates Stable Diffusion by varying parameters across frames. Beat data maps directly to Deforum keyframe syntax:

```python
def audio_to_deforum_keyframes(analysis: AudioAnalysis, section: SectionSpec) -> str:
    """Convert audio analysis to Deforum animation keyframe string."""
    keyframes = {}

    for i, beat_time in enumerate(analysis.rhythm.beats):
        frame = int(beat_time * fps)
        energy = analysis.spectral.rms[nearest_index(analysis.spectral.times, beat_time)]

        # Zoom in on bass-heavy beats
        bass = analysis.spectral.energy_bands.bass[nearest_index(...)]
        keyframes[frame] = {
            "zoom": 1.0 + (bass * 0.05),           # subtle zoom on bass
            "angle": energy * 2.0,                   # rotation on energy
            "translation_x": random_walk(),
            "translation_y": random_walk(),
            "strength_schedule": 0.5 + energy * 0.2  # more denoising on loud parts
        }

    # Prompt changes at section boundaries
    # Different prompt for each section
    prompt_schedule = {}
    for section in render_spec.sections:
        frame = int(section.start_time * fps)
        prompt_schedule[frame] = section.ai_prompt

    return format_deforum_schedule(keyframes, prompt_schedule)
```

### AnimateDiff Integration

AnimateDiff generates temporally consistent clips (16-32 frames). For a full song:

```
1. Divide each section into clips (2-4 seconds each)
2. For each clip:
   a. Generate with AnimateDiff (prompt from section spec)
   b. Apply motion LoRA matching the section's energy
   c. Use IP-Adapter for visual consistency with previous clip
3. Stitch clips with frame interpolation (RIFE or FILM)
4. Apply audio-reactive post-processing in Remotion
```

### Video Generation Models (Alternative)

| Model | Clip Length | Quality | Speed |
|-------|-----------|---------|-------|
| **AnimateDiff** | 16-32 frames (~1-2s) | Good consistency | ~30s/clip (GPU) |
| **Stable Video Diffusion** | 14-25 frames (~1-2s) | High quality, limited motion | ~45s/clip |
| **CogVideoX** | 48 frames (~2s at 24fps) | High quality | ~60s/clip |
| **Hunyuan Video** | Up to 5s | High quality | ~90s/clip |
| **LTX Video** | Variable | Fast generation | ~20s/clip |

### ComfyUI as Orchestrator

ComfyUI provides a node-based workflow system with a REST API. The server sends workflow JSON to ComfyUI, which executes the image/video generation pipeline:

```python
async def generate_via_comfyui(workflow: dict, params: dict) -> str:
    """Submit a generation job to ComfyUI API."""
    # Inject parameters into workflow nodes
    workflow = inject_params(workflow, params)

    # Queue the prompt
    async with aiohttp.ClientSession() as session:
        resp = await session.post(
            f"{COMFYUI_URL}/prompt",
            json={"prompt": workflow}
        )
        prompt_id = (await resp.json())["prompt_id"]

    # Poll for completion
    return await poll_for_result(prompt_id)
```

---

## Cost Estimates

### Tier 2 (AI Keyframes)

| Provider | Cost Per Image | Images Per Song | Cost Per Song |
|----------|---------------|-----------------|---------------|
| DALL-E 3 (1024x1024) | $0.04 | 15 | $0.60 |
| DALL-E 3 (1792x1024) | $0.08 | 15 | $1.20 |
| Stability AI Ultra | $0.08 | 15 | $1.20 |
| Self-hosted SD (RunPod A100) | ~$0.002 | 15 | ~$0.03 |

### Tier 3 (Full AI Video)

| Approach | GPU Time | Cost (Cloud GPU) |
|----------|----------|-------------------|
| Deforum (3-min song, 30fps) | ~30-60 min on A100 | $1.50-$3.00 |
| AnimateDiff clips | ~15-30 min on A100 | $0.75-$1.50 |
| Full quality pipeline | ~45-90 min on A100 | $2.25-$4.50 |

Self-hosting amortizes GPU cost significantly for high volume.

---

## Fallback Strategy

If AI image generation fails for any section:

1. **Retry once** with simplified prompt
2. **Fall back to Tier 1** procedural visuals for that section
3. **Notify user** in chat: "AI image generation failed for [section]. Using procedural visuals as fallback. You can retry or provide a different prompt."
4. Never block the entire render due to a single section's AI failure
